---
title: "P8122-Homework3-yz4184"
author: "Yunlin Zhou"
date: '2022-10-09'
output: word_document
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r}
# data import
set.seed(124)
n <- 16
p_C <- 1/5
C <- rbinom(n,1,p_C)
theta0 <- 1/2
theta1 <- -1/5
p_A <- theta0+theta1*C
A <- rbinom(n,1,p_A)
beta0 <- 110
beta1 <- 20
beta2 <- 5
sigma_Y <- 1
mu_Y <- beta0+beta1*C+beta2*A
Y <- rnorm(n,mu_Y, sigma_Y)
```

# Question a

p: The baseline covariate C (obesity) follows Bernoulli distribution. p is the probability of C taking 1 (with obesity).

theta0: The original probability of assigning the units to treatment group (light) if there were no baseline covariate C.

theta1: The probability of assigning the units with obesity to treatment group (light) is 1/5 times lower than the units without obesity on average.

beta0: The mean of baseline glucose without baseline covariate C and treatment assignment A.

beta1: Intervening to increase C (obesity) by one unit will, on average, increase the outcome Y (glucose) by beta1 units.

beta2: Intervening to increase A (light) by one unit will, on average, increase the outcome Y (glucose) by beta2 units.

# Question c

## For randimized study

```{r}
data = read.csv("./light.csv") %>% 
  janitor::clean_names()

bright_dark_df = data %>% 
  filter(light %in% c("LD", "LL"))

bright_dark_df = bright_dark_df  %>%
  select(bm_gain, light, everything())%>%
  rename( Y_obs = bm_gain,
          A = light)

Y_obs = bright_dark_df$Y_obs
A = bright_dark_df$A
A = as.factor(A)
```


```{r}
E_Y1_r = mean(Y_obs[A == "LL"])

E_Y0_r = mean(Y_obs[A == "LD"])
```

According to the g-formula: E[Y1] = `r E_Y1_r`  E[Y0] = `r E_Y0_r`


## For observational study





# Question d

```{r}
mice_df = cbind(Y, A, C)%>%
  as.data.frame()
```


```{r}
mice_df$interv <- -1 

interv0 <- mice_df
interv0$interv <- 0
interv0$A <- 0
interv0$Y<- NA

interv1 <- mice_df 
interv1$interv <- 1
interv1$A <- 1
interv1$Y<- NA

onesample <- rbind(mice_df, interv0, interv1) 
```


```{r}
std <- lm(
  Y ~ A + C,
  data = onesample
)
onesample$predicted_meanY <- predict(std, onesample)
```


```{r}
mean_0_c =mean(onesample[which(onesample$interv == 0), ]$predicted_meanY)


mean_1_c = mean(onesample[which(onesample$interv == 1),]$predicted_meanY)
```


```{r}
mean_1_c - mean_0_c
```

E[Y |A = 1] âˆ’ E[Y |A = 0] = `r mean_1_c - mean_0_c`

# Question e

```{r}
df = onesample %>%
  filter(interv == -1)

mean_1 = mean(df[which(df$A == 1), ]$predicted_meanY)
mean_0 = mean(df[which(df$A == 0), ]$predicted_meanY)
mean_1 - mean_0
```


## another try

```{r}
mice_df_2 = cbind(mu_Y, A, C)%>%
  as.data.frame()
```


```{r}
mean_0_y =mean(mice_df_2[which(mice_df_2$A == 0), ]$mu_Y)


mean_1_y = mean(mice_df_2[which(mice_df_2$A == 1),]$mu_Y)
```


```{r}
mean_1_y- mean_0_y
```


# Question f

1. The linear regression to be correctly specified
2. All confounders are included in the new covariates.

































































